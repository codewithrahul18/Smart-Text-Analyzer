# A comprehensive NLP tool for text analysis using AI and machine learning.

# %%
# Install all required dependencies
!pip install -q transformers torch spacy textblob wordcloud plotly nltk scikit-learn matplotlib seaborn pandas numpy
!pip install -q fpdf reportlab gtts pygame gensim
!python -m spacy download en_core_web_sm -q
!pip install -q ipywidgets

# %%
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
from collections import Counter
import string
from textblob import TextBlob
from transformers import pipeline
import torch
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

# Enhanced dependencies
from fpdf import FPDF
import base64
from datetime import datetime
import tempfile
import os

# Download NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('punkt_tab', quiet=True)

print("âœ… All dependencies installed and ready!")

# Add custom CSS styling
display(HTML("""
<style>
    .header-title {
        color: #1f77b4;
        font-size: 2.5em;
        font-weight: bold;
        text-align: center;
        margin-bottom: 10px;
    }
    .creator-name {
        color: #ff6b6b;
        font-size: 1.5em;
        font-weight: bold;
        text-align: center;
        margin-bottom: 20px;
    }
    .section-header {
        background: linear-gradient(90deg, #1f77b4, #ff6b6b);
        color: white;
        padding: 10px;
        border-radius: 10px;
        margin: 20px 0;
        text-align: center;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
</style>
"""))

# Display header
display(HTML("""
<div class="header-title">ğŸ” Smart Text Analyzer</div>
<div class="creator-name">Created by Rahul Chauhan</div>
"""))

# %%
# Display feature overview
display(HTML("""
<div class="section-header">ğŸš€ Features Overview</div>
"""))

features = [
    "ğŸ“Š Basic Text Statistics & Metrics",
    "ğŸ˜Š Sentiment Analysis (TextBlob + Transformers)",
    "ğŸ·ï¸ Named Entity Recognition",
    "ğŸ“ Text Summarization (Extractive & Abstractive)",
    "ğŸ“ˆ Beautiful Visualizations & Word Clouds",
    "ğŸ¯ Advanced Text Classification",
    "ğŸ§  Topic Modeling with LDA",
    "ğŸ“„ PDF Report Export",
    "ğŸ’¾ Comprehensive Report Generation"
]

for feature in features:
    display(HTML(f"<div style='padding: 8px; font-size: 1.1em;'>âœ… {feature}</div>"))

# Enhanced PDF Generator Class
class PDFReportGenerator:
    def __init__(self):
        pass
    
    def generate_comprehensive_report(self, text, analysis_results, filename="text_analysis_report.pdf"):
        """Generate comprehensive PDF report"""
        try:
            # Create PDF instance
            pdf = FPDF()
            pdf.add_page()
            
            # Set font for the title
            pdf.set_font("Arial", 'B', 16)
            
            # Title
            pdf.cell(200, 10, "Smart Text Analyzer - Comprehensive Report", ln=True, align='C')
            pdf.ln(10)
            
            # Metadata
            pdf.set_font("Arial", size=12)
            pdf.cell(200, 10, f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", ln=True)
            pdf.cell(200, 10, "Created by: Rahul Chauhan", ln=True)
            pdf.ln(10)
            
            # Basic Statistics
            pdf.set_font("Arial", 'B', 14)
            pdf.cell(200, 10, "Basic Text Statistics", ln=True)
            pdf.set_font("Arial", size=10)
            
            stats = analysis_results['basic_statistics']
            for key, value in stats.items():
                formatted_key = key.replace('_', ' ').title()
                pdf.cell(100, 8, f"{formatted_key}:", ln=0)
                pdf.cell(90, 8, f"{value:.2f}", ln=True)
            
            pdf.ln(10)
            
            # Sentiment Analysis
            pdf.set_font("Arial", 'B', 14)
            pdf.cell(200, 10, "Sentiment Analysis", ln=True)
            pdf.set_font("Arial", size=10)
            
            sentiment = analysis_results['sentiment_analysis']
            pdf.cell(200, 8, f"Overall Consensus: {sentiment['consensus'].title()}", ln=True)
            pdf.cell(200, 8, f"TextBlob Polarity: {sentiment['textblob']['polarity']:.3f}", ln=True)
            pdf.cell(200, 8, f"TextBlob Subjectivity: {sentiment['textblob']['subjectivity']:.3f}", ln=True)
            pdf.cell(200, 8, f"Transformers Sentiment: {sentiment['transformers']['label']}", ln=True)
            pdf.cell(200, 8, f"Transformers Confidence: {sentiment['transformers']['score']:.3f}", ln=True)
            
            pdf.ln(10)
            
            # Entity Information
            pdf.set_font("Arial", 'B', 14)
            pdf.cell(200, 10, "Entity Information", ln=True)
            pdf.set_font("Arial", size=10)
            
            pdf.cell(200, 8, f"Total entities found: {analysis_results['entities_found']}", ln=True)
            if analysis_results['entity_types']:
                pdf.cell(200, 8, f"Entity types: {', '.join(analysis_results['entity_types'])}", ln=True)
            
            pdf.ln(10)
            
            # Word Frequency
            pdf.set_font("Arial", 'B', 14)
            pdf.cell(200, 10, "Top Words", ln=True)
            pdf.set_font("Arial", size=10)
            
            word_freq = analysis_results['word_frequency']
            top_words = dict(list(word_freq.items())[:10])
            for word, freq in top_words.items():
                pdf.cell(100, 8, f"{word}:", ln=0)
                pdf.cell(90, 8, f"{freq}", ln=True)
            
            # Save PDF
            pdf.output(filename)
            return filename
            
        except Exception as e:
            return f"PDF generation failed: {str(e)}"

# Text Analysis Classes (keep all your existing classes as they are)
class BasicTextAnalyzer:
    def __init__(self, text):
        self.text = text
        self.stop_words = set(stopwords.words('english'))

    def get_basic_stats(self):
        """Get basic text statistics"""
        try:
            words = word_tokenize(self.text)
            sentences = sent_tokenize(self.text)

            words_clean = [word.lower() for word in words if word.isalnum()]

            stats = {
                'char_count': len(self.text),
                'word_count': len(words_clean),
                'sentence_count': len(sentences),
                'avg_word_length': sum(len(word) for word in words_clean) / len(words_clean) if words_clean else 0,
                'avg_sentence_length': len(words_clean) / len(sentences) if sentences else 0,
                'unique_words': len(set(words_clean)),
                'lexical_diversity': len(set(words_clean)) / len(words_clean) if words_clean else 0,
                'paragraph_count': len([p for p in self.text.split('\n\n') if p.strip()]),
                'reading_time_minutes': len(words_clean) / 200
            }
            return stats
        except Exception as e:
            print(f"Error in get_basic_stats: {e}")
            return {
                'char_count': len(self.text),
                'word_count': len(self.text.split()),
                'sentence_count': len(self.text.split('.')),
                'avg_word_length': 5,
                'avg_sentence_length': 15,
                'unique_words': len(set(self.text.split())),
                'lexical_diversity': 0.5,
                'paragraph_count': len([p for p in self.text.split('\n\n') if p.strip()]),
                'reading_time_minutes': len(self.text.split()) / 200
            }

    def get_word_frequency(self, top_n=20):
        """Get most frequent words"""
        try:
            words = [word.lower() for word in word_tokenize(self.text) if word.isalnum()]
            words_filtered = [word for word in words if word not in self.stop_words and len(word) > 2]
            freq_dist = FreqDist(words_filtered)
            return dict(freq_dist.most_common(top_n))
        except Exception as e:
            print(f"Error in get_word_frequency: {e}")
            words = [word.lower() for word in self.text.split() if word.isalnum()]
            words_filtered = [word for word in words if word not in self.stop_words and len(word) > 2]
            return dict(Counter(words_filtered).most_common(top_n))

    def get_pos_tags(self):
        """Get Part-of-Speech tags distribution"""
        try:
            words = word_tokenize(self.text)
            pos_tags = nltk.pos_tag(words)
            tag_counts = Counter(tag for word, tag in pos_tags)
            return dict(tag_counts.most_common(15))
        except Exception as e:
            print(f"Error in get_pos_tags: {e}")
            return {}

class SentimentAnalyzer:
    def __init__(self):
        try:
            self.classifier = pipeline("sentiment-analysis",
                                     model="distilbert-base-uncased-finetuned-sst-2-english")
        except:
            self.classifier = None
            print("âš ï¸ Transformers sentiment analyzer not available. Using TextBlob only.")

    def analyze_with_textblob(self, text):
        """Analyze sentiment using TextBlob"""
        try:
            blob = TextBlob(text)
            sentiment = {
                'polarity': blob.sentiment.polarity,
                'subjectivity': blob.sentiment.subjectivity,
                'sentiment': 'positive' if blob.sentiment.polarity > 0.1 else
                            'negative' if blob.sentiment.polarity < -0.1 else 'neutral'
            }
            return sentiment
        except Exception as e:
            print(f"Error in analyze_with_textblob: {e}")
            return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'neutral'}

    def analyze_with_transformers(self, text):
        """Analyze sentiment using Transformers"""
        if self.classifier is None:
            return {'label': 'N/A', 'score': 0, 'sentiment': 'neutral'}

        try:
            if len(text) > 512:
                text = text[:512]

            result = self.classifier(text)[0]
            return {
                'label': result['label'],
                'score': result['score'],
                'sentiment': result['label'].lower()
            }
        except Exception as e:
            print(f"Error in analyze_with_transformers: {e}")
            return {'label': 'ERROR', 'score': 0, 'sentiment': 'neutral'}

    def get_detailed_sentiment(self, text):
        """Get comprehensive sentiment analysis"""
        textblob_result = self.analyze_with_textblob(text)
        transformer_result = self.analyze_with_transformers(text)

        return {
            'textblob': textblob_result,
            'transformers': transformer_result,
            'consensus': 'positive' if (textblob_result['polarity'] > 0.1 and
                                      transformer_result['sentiment'] == 'positive') else
                        'negative' if (textblob_result['polarity'] < -0.1 and
                                     transformer_result['sentiment'] == 'negative') else 'neutral'
        }

class EntityExtractor:
    def __init__(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            self.nlp = None
            print("âš ï¸ spaCy model not available. Entity extraction disabled.")

    def extract_entities_spacy(self, text):
        """Extract named entities using spaCy"""
        if self.nlp is None:
            return []

        try:
            doc = self.nlp(text)
            entities = []

            for ent in doc.ents:
                entities.append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char
                })

            return entities
        except Exception as e:
            print(f"Error in extract_entities_spacy: {e}")
            return []

    def get_entity_summary(self, text):
        """Get comprehensive entity extraction"""
        spacy_entities = self.extract_entities_spacy(text)

        entity_summary = {}
        for entity in spacy_entities:
            entity_type = entity['label']
            if entity_type not in entity_summary:
                entity_summary[entity_type] = []
            entity_summary[entity_type].append(entity['text'])

        return {
            'spacy_entities': spacy_entities,
            'entity_summary': entity_summary
        }

class TextSummarizer:
    def __init__(self):
        try:
            self.summarizer = pipeline("summarization",
                                     model="facebook/bart-large-cnn")
        except:
            self.summarizer = None
            print("âš ï¸ Transformers summarizer not available. Using extractive summarization only.")

    def extractive_summarization(self, text, num_sentences=3):
        """Extractive summarization using TF-IDF"""
        try:
            sentences = sent_tokenize(text)

            if len(sentences) <= num_sentences:
                return text

            vectorizer = TfidfVectorizer(stop_words='english')
            sentence_vectors = vectorizer.fit_transform(sentences)

            sentence_scores = sentence_vectors.sum(axis=1)
            sentence_ranking = sentence_scores.argsort(axis=0)[::-1]

            top_sentences = sentence_ranking[:num_sentences]
            top_sentences = sorted(top_sentences.flatten().tolist())

            summary = ' '.join([sentences[i] for i in top_sentences])
            return summary
        except Exception as e:
            print(f"Error in extractive_summarization: {e}")
            sentences = text.split('.')
            return ". ".join(sentences[:num_sentences]) + "."

    def abstractive_summarization(self, text, max_length=150, min_length=30):
        """Abstractive summarization using Transformers"""
        if self.summarizer is None:
            return "Abstractive summarization not available."

        try:
            if len(text) > 1024:
                text = text[:1024]

            summary = self.summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
            return summary[0]['summary_text']
        except Exception as e:
            return f"Summarization error: {str(e)}"

    def summarize_text(self, text, method='abstractive'):
        """Comprehensive text summarization"""
        if method == 'extractive':
            return self.extractive_summarization(text)
        elif method == 'abstractive':
            return self.abstractive_summarization(text)
        else:
            raise ValueError("Method must be 'extractive' or 'abstractive'")

class TextVisualizer:
    def __init__(self):
        plt.style.use('default')
        sns.set_palette("husl")

    def plot_word_frequency(self, word_freq, title="Most Frequent Words"):
        """Plot word frequency distribution"""
        fig, ax = plt.subplots(figsize=(12, 6))
        words, frequencies = zip(*word_freq.items())

        colors = plt.cm.viridis(np.linspace(0, 1, len(words)))
        bars = ax.bar(words, frequencies, color=colors)
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlabel('Words')
        ax.set_ylabel('Frequency')
        ax.tick_params(axis='x', rotation=45)

        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom')

        plt.tight_layout()
        return fig

    def generate_wordcloud(self, text, max_words=100):
        """Generate word cloud from text"""
        fig, ax = plt.subplots(figsize=(12, 6))
        wordcloud = WordCloud(width=800, height=400,
                            background_color='white',
                            max_words=max_words,
                            colormap='viridis').generate(text)

        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Word Cloud - Created by Rahul Chauhan', fontsize=16, fontweight='bold')
        return fig

    def plot_sentiment_gauge(self, polarity, subjectivity):
        """Create sentiment gauge charts"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # Polarity gauge
        color_polarity = 'green' if polarity > 0.1 else 'red' if polarity < -0.1 else 'gray'
        ax1.barh(['Polarity'], [polarity], color=color_polarity)
        ax1.set_xlim(-1, 1)
        ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        ax1.set_title('Sentiment Polarity')
        ax1.text(polarity, 0, f'{polarity:.3f}', ha='center', va='center', fontweight='bold')

        # Subjectivity gauge
        ax2.barh(['Subjectivity'], [subjectivity], color='orange')
        ax2.set_xlim(0, 1)
        ax2.set_title('Subjectivity')
        ax2.text(subjectivity, 0, f'{subjectivity:.3f}', ha='center', va='center', fontweight='bold')

        plt.tight_layout()
        return fig

class TopicModeler:
    def __init__(self):
        pass
    
    def extract_topics(self, text, num_topics=3, num_words=5):
        """Extract topics using LDA"""
        try:
            # Preprocess text
            sentences = sent_tokenize(text)
            tokenized_sentences = [word_tokenize(sent.lower()) for sent in sentences]
            
            # Remove stopwords and short tokens
            stop_words = set(stopwords.words('english'))
            processed_sentences = [
                [token for token in sent if token not in stop_words and len(token) > 2 and token.isalnum()]
                for sent in tokenized_sentences
            ]
            
            # Remove empty sentences
            processed_sentences = [sent for sent in processed_sentences if sent]
            
            if len(processed_sentences) < 2:
                return ["Insufficient text for topic modeling"]
            
            # Create dictionary and corpus
            from gensim import corpora
            dictionary = corpora.Dictionary(processed_sentences)
            corpus = [dictionary.doc2bow(sent) for sent in processed_sentences]
            
            # Train LDA model
            from gensim.models import LdaModel
            lda_model = LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=num_topics,
                random_state=42,
                passes=10,
                alpha='auto',
                per_word_topics=True
            )
            
            # Extract topics
            topics = []
            for idx, topic in lda_model.print_topics(-1, num_words=num_words):
                topics.append(f"Topic {idx + 1}: {topic}")
            
            return topics
            
        except Exception as e:
            return [f"Topic modeling error: {str(e)}"]

class AdvancedTextAnalysis:
    def __init__(self):
        try:
            self.zero_shot_classifier = pipeline("zero-shot-classification")
        except:
            self.zero_shot_classifier = None

    def zero_shot_classification(self, text, categories):
        """Classify text into custom categories"""
        if self.zero_shot_classifier is None:
            return "Zero-shot classification not available"

        try:
            result = self.zero_shot_classifier(text, categories)
            return {
                'labels': result['labels'],
                'scores': result['scores']
            }
        except:
            return "Classification error"

    def text_complexity(self, text):
        """Analyze text complexity"""
        words = text.split()
        sentences = text.split('.')

        # Calculate various complexity metrics
        avg_word_length = np.mean([len(word) for word in words if word.isalnum()])
        avg_sentence_length = len(words) / len(sentences) if sentences else 0

        # Flesch Reading Ease (simplified)
        complex_words = [word for word in words if len(word) > 6 and word.isalnum()]
        complexity_ratio = len(complex_words) / len(words) if words else 0

        return {
            'average_word_length': avg_word_length,
            'average_sentence_length': avg_sentence_length,
            'complex_word_ratio': complexity_ratio,
            'readability_level': 'Advanced' if complexity_ratio > 0.2 else
                               'Intermediate' if complexity_ratio > 0.1 else 'Basic'
        }

# Initialize all analyzers
display(HTML("<div class='section-header'>ğŸ› ï¸ Initializing Analyzers</div>"))
sentiment_analyzer = SentimentAnalyzer()
entity_extractor = EntityExtractor()
text_summarizer = TextSummarizer()
visualizer = TextVisualizer()
pdf_generator = PDFReportGenerator()
topic_modeler = TopicModeler()
advanced_analyzer = AdvancedTextAnalysis()

display(HTML("<div class='success-box'>âœ… All analyzers initialized successfully!</div>"))

# %%
# Create sample texts
sample_texts = {
    "Product Review": """
    I recently purchased the XYZ Smartphone and I'm absolutely thrilled with it!
    The camera quality is exceptional, capturing stunning photos even in low light.
    The battery life lasts all day with heavy usage, which is impressive.
    However, I did notice the phone gets slightly warm during extended gaming sessions.
    The display is vibrant and the interface is smooth. Overall, it's a fantastic device
    that exceeds expectations. I would highly recommend it to anyone looking for a
    premium smartphone experience. The customer service was also excellent when I had questions.
    """,

    "News Article": """
    Scientists at the International Research Institute have made a groundbreaking
    discovery in renewable energy technology. The team, led by Dr. Sarah Chen,
    has developed a new solar panel material that increases energy efficiency by 45%.
    This innovation could significantly reduce our reliance on fossil fuels and
    accelerate the transition to clean energy. The research was conducted in
    collaboration with Stanford University and MIT. Funding for the project was
    provided by the National Science Foundation. The new technology is expected
    to be commercially available within two years, potentially revolutionizing
    how we harness solar power globally.
    """,

    "Technical Document": """
    The implementation of convolutional neural networks (CNNs) has revolutionized
    computer vision applications. These deep learning models utilize multiple layers
    to automatically extract hierarchical features from input images. The architecture
    typically consists of convolutional layers, pooling layers, and fully connected
    layers. Batch normalization and dropout techniques are commonly employed to
    prevent overfitting. The model is trained using backpropagation with stochastic
    gradient descent optimization. Performance metrics including accuracy, precision,
    recall, and F1-score are used for evaluation. Recent advancements in transformer
    architectures have further improved performance in various computer vision tasks.
    """,

    "Business Report": """
    Quarterly financial results for Q4 2024 show significant growth across all sectors.
    Revenue increased by 23% compared to the same period last year, reaching $450 million.
    The technology division, led by Vice President Michael Johnson, demonstrated
    exceptional performance with a 35% growth rate. Market expansion into Southeast Asia
    has proven successful, with operations in Singapore and Malaysia exceeding targets.
    However, challenges remain in the European market due to regulatory changes.
    The company remains optimistic about future growth prospects and plans to invest
    heavily in research and development throughout the next fiscal year.
    """
}

# Create widgets for user input
display(HTML("<div class='section-header'>ğŸ“ Text Input Section</div>"))
display(HTML("<div class='info-box'>Choose how you want to input text for analysis:</div>"))

input_method = widgets.RadioButtons(
    options=['Enter Text', 'Use Sample Text'],
    description='Input Method:',
    disabled=False,
    style={'description_width': 'initial'}
)

sample_selector = widgets.Dropdown(
    options=list(sample_texts.keys()),
    description='Sample Type:',
    disabled=False,
    style={'description_width': 'initial'}
)

text_input = widgets.Textarea(
    value='',
    placeholder='Enter your text here for analysis...\n\nYou can paste articles, reviews, documents, or any text you want to analyze.',
    description='Your Text:',
    layout=widgets.Layout(width='100%', height='200px')
)

analyze_button = widgets.Button(
    description='ğŸš€ Analyze Text',
    button_style='success',
    layout=widgets.Layout(width='200px', height='40px'),
    style={'button_color': '#28a745'}
)

# Display widgets
display(widgets.VBox([
    widgets.HTML("<h3>ğŸ“‹ Input Configuration</h3>"),
    input_method,
    sample_selector,
    widgets.HTML("<h3>âœï¸ Text Input</h3>"),
    text_input,
    analyze_button
]))

# Widget interaction logic
def on_input_method_change(change):
    if change['new'] == 'Use Sample Text':
        text_input.value = sample_texts[sample_selector.value]

def on_sample_selector_change(change):
    if input_method.value == 'Use Sample Text':
        text_input.value = sample_texts[change['new']]

input_method.observe(on_input_method_change, names='value')
sample_selector.observe(on_sample_selector_change, names='value')

# %%
# Create output areas
output_area = widgets.Output()

def analyze_text(b):
    with output_area:
        clear_output()

        text = text_input.value.strip()
        if not text:
            display(HTML("<div style='color: red; font-weight: bold;'>âŒ Please enter some text to analyze.</div>"))
            return

        display(HTML("<div class='success-box'>ğŸ” Analyzing text... Please wait while we process your text.</div>"))

        # Initialize basic analyzer
        text_analyzer = BasicTextAnalyzer(text)

        # Create tabs
        tab_contents = ['ğŸ“Š Basic Stats', 'ğŸ˜Š Sentiment', 'ğŸ·ï¸ Entities', 'ğŸ“ Summary', 'ğŸ“ˆ Visualizations', 'ğŸ§  Advanced']
        tabs = widgets.Tab()
        tabs.children = [widgets.Output() for _ in tab_contents]
        for i, title in enumerate(tab_contents):
            tabs.set_title(i, title)

        display(tabs)

        # Tab 1: Basic Statistics
        with tabs.children[0]:
            display(HTML("<h2>ğŸ“Š Basic Text Statistics</h2>"))

            stats = text_analyzer.get_basic_stats()
            word_freq = text_analyzer.get_word_frequency()
            pos_tags = text_analyzer.get_pos_tags()

            columns_container = widgets.HBox([widgets.Output(), widgets.Output()])
            display(columns_container)

            with columns_container.children[0]:
                display(HTML("<h3>ğŸ“ˆ Key Metrics</h3>"))
                stats_df = pd.DataFrame([stats])
                stats_display = stats_df.T.round(3)
                stats_display.columns = ['Value']
                stats_display.index = [idx.replace('_', ' ').title() for idx in stats_display.index]
                display(stats_display)

            with columns_container.children[1]:
                display(HTML("<h3>ğŸ”¤ Word Frequency</h3>"))
                if word_freq:
                    freq_df = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])
                    display(freq_df.head(10))
                else:
                    display(HTML("<div>No significant words found.</div>"))

            if pos_tags:
                display(HTML("<h3>ğŸ“ Part-of-Speech Tags</h3>"))
                pos_df = pd.DataFrame(list(pos_tags.items()), columns=['POS Tag', 'Count'])
                display(pos_df)

        # Tab 2: Sentiment Analysis
        with tabs.children[1]:
            display(HTML("<h2>ğŸ˜Š Sentiment Analysis</h2>"))

            sentiment = sentiment_analyzer.get_detailed_sentiment(text)

            sentiment_columns = widgets.HBox([widgets.Output(), widgets.Output()])
            display(sentiment_columns)

            with sentiment_columns.children[0]:
                display(HTML("<h3>ğŸ“Š TextBlob Analysis</h3>"))
                tb_data = sentiment['textblob']
                tb_df = pd.DataFrame([tb_data])
                display(tb_df)

                polarity = tb_data['polarity']
                if polarity > 0.3:
                    sentiment_text = "Strongly Positive ğŸ˜Š"
                elif polarity > 0.1:
                    sentiment_text = "Positive ğŸ™‚"
                elif polarity > -0.1:
                    sentiment_text = "Neutral ğŸ˜"
                elif polarity > -0.3:
                    sentiment_text = "Negative ğŸ™"
                else:
                    sentiment_text = "Strongly Negative ğŸ˜"

                display(HTML(f"<div class='info-box'><strong>Interpretation:</strong> {sentiment_text}</div>"))

            with sentiment_columns.children[1]:
                display(HTML("<h3>ğŸ¤– Transformers Analysis</h3>"))
                if sentiment['transformers']['label'] != 'N/A':
                    tf_data = sentiment['transformers']
                    tf_df = pd.DataFrame([tf_data])
                    display(tf_df)

                    confidence = tf_data['score']
                    display(HTML(f"<div style='margin-top: 10px;'><strong>Confidence Level:</strong></div>"))
                    display(HTML(f"<progress value='{confidence}' max='1' style='width: 100%; height: 20px;'></progress>"))
                    display(HTML(f"<div style='text-align: center;'>{confidence:.1%}</div>"))
                else:
                    display(HTML("<div>Transformers analysis not available</div>"))

            display(HTML("<h3>ğŸ¯ Overall Consensus</h3>"))
            consensus_emoji = "âœ…" if sentiment['consensus'] == 'positive' else "âš ï¸" if sentiment['consensus'] == 'neutral' else "âŒ"
            display(HTML(f"<div class='success-box' style='text-align: center; font-size: 1.2em;'>"
                        f"<strong>{sentiment['consensus'].title()} Sentiment</strong> {consensus_emoji}</div>"))

            display(HTML("<h3>ğŸ“Š Sentiment Visualization</h3>"))
            fig = visualizer.plot_sentiment_gauge(
                sentiment['textblob']['polarity'],
                sentiment['textblob']['subjectivity']
            )
            plt.show()

        # Tab 3: Entity Extraction
        with tabs.children[2]:
            display(HTML("<h2>ğŸ·ï¸ Named Entity Recognition</h2>"))

            entities = entity_extractor.get_entity_summary(text)

            if entities['spacy_entities']:
                display(HTML("<h3>ğŸ” Extracted Entities</h3>"))
                entity_df = pd.DataFrame(entities['spacy_entities'])
                display(entity_df)

                display(HTML("<h3>ğŸ“‹ Entity Summary by Type</h3>"))
                for entity_type, entity_list in entities['entity_summary'].items():
                    unique_entities = list(set(entity_list))
                    with widgets.Output():
                        display(HTML(f"<details><summary><strong>{entity_type}</strong> ({len(unique_entities)} entities)</summary>"
                                    f"<div style='padding: 10px;'>{', '.join(unique_entities)}</div></details>"))

                display(HTML("<h3>ğŸ“ˆ Entity Statistics</h3>"))
                entity_counts = {k: len(v) for k, v in entities['entity_summary'].items()}
                if entity_counts:
                    count_df = pd.DataFrame(list(entity_counts.items()),
                                          columns=['Entity Type', 'Count'])
                    fig, ax = plt.subplots(figsize=(10, 6))
                    colors = plt.cm.Set3(np.linspace(0, 1, len(count_df)))
                    ax.bar(count_df['Entity Type'], count_df['Count'], color=colors)
                    ax.set_title('Entity Count by Type - Created by Rahul Chauhan')
                    ax.set_ylabel('Count')
                    plt.xticks(rotation=45)
                    plt.tight_layout()
                    plt.show()
            else:
                display(HTML("<div class='info-box'>âŒ No named entities found in the text, or entity extraction is not available.</div>"))

        # Tab 4: Text Summary
        with tabs.children[3]:
            display(HTML("<h2>ğŸ“ Text Summarization</h2>"))

            summary_columns = widgets.HBox([widgets.Output(), widgets.Output()])
            display(summary_columns)

            with summary_columns.children[0]:
                display(HTML("<h3>ğŸ“‹ Extractive Summary</h3>"))
                display(HTML("<p><em>Selects important sentences from the original text</em></p>"))

                num_sentences = widgets.IntSlider(value=3, min=2, max=10, description='Sentences:')
                display(num_sentences)

                extractive_summary = text_summarizer.extractive_summarization(text, num_sentences.value)

                display(HTML(f"""
                <div style='
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 20px;
                    border-radius: 10px;
                    border-left: 6px solid #ff6b6b;
                    margin: 15px 0;
                    font-size: 1.1em;
                    line-height: 1.6;
                    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                '>
                    <div style='font-weight: bold; margin-bottom: 10px; font-size: 1.2em;'>ğŸ“„ Extractive Summary:</div>
                    {extractive_summary}
                </div>
                """))

                stats_html = f"""
                <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; text-align: center; margin: 20px 0;">
                    <div style="background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 15px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                        <strong>Original Words</strong><br>
                        <span style="font-size: 1.5em; font-weight: bold;">{len(text.split())}</span>
                    </div>
                    <div style="background: linear-gradient(135deg, #f093fb, #f5576c); color: white; padding: 15px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                        <strong>Summary Words</strong><br>
                        <span style="font-size: 1.5em; font-weight: bold;">{len(extractive_summary.split())}</span>
                    </div>
                    <div style="background: linear-gradient(135deg, #4facfe, #00f2fe); color: white; padding: 15px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                        <strong>Compression</strong><br>
                        <span style="font-size: 1.5em; font-weight: bold;">{len(extractive_summary) / len(text) * 100:.1f}%</span>
                    </div>
                </div>
                """
                display(HTML(stats_html))

            with summary_columns.children[1]:
                display(HTML("<h3>ğŸ§  Abstractive Summary</h3>"))
                display(HTML("<p><em>Generates new sentences that capture the essence</em></p>"))

                abstractive_summary = text_summarizer.summarize_text(text, 'abstractive')

                if "not available" in abstractive_summary or "error" in abstractive_summary.lower():
                    display(HTML(f"""
                    <div style='
                        background: linear-gradient(135deg, #ffd89b, #19547b);
                        color: white;
                        padding: 20px;
                        border-radius: 10px;
                        border-left: 6px solid #ff6b6b;
                        margin: 15px 0;
                        font-size: 1.1em;
                        line-height: 1.6;
                        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                    '>
                        <div style='font-weight: bold; margin-bottom: 10px; font-size: 1.2em;'>ğŸ¤– Abstractive Summary:</div>
                        {abstractive_summary}
                    </div>
                    """))
                    display(HTML("<div class='info-box'>âš ï¸ Abstractive summarization requires additional setup or the text might be too short.</div>"))
                else:
                    display(HTML(f"""
                    <div style='
                        background: linear-gradient(135deg, #ff9a9e, #fecfef);
                        color: #333;
                        padding: 20px;
                        border-radius: 10px;
                        border-left: 6px solid #e91e63;
                        margin: 15px 0;
                        font-size: 1.1em;
                        line-height: 1.6;
                        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                    '>
                        <div style='font-weight: bold; margin-bottom: 10px; font-size: 1.2em; color: #d81b60;'>ğŸ¤– Abstractive Summary:</div>
                        {abstractive_summary}
                    </div>
                    """))

        # Tab 5: Visualizations
        with tabs.children[4]:
            display(HTML("<h2>ğŸ“ˆ Text Visualizations</h2>"))

            word_freq = text_analyzer.get_word_frequency()

            if word_freq:
                display(HTML("<h3>ğŸ“Š Word Frequency Distribution</h3>"))
                fig1 = visualizer.plot_word_frequency(word_freq)
                plt.show()

                display(HTML("<h3>â˜ï¸ Word Cloud</h3>"))
                fig2 = visualizer.generate_wordcloud(text)
                plt.show()
            else:
                display(HTML("<div class='info-box'>âŒ Not enough text data for visualizations.</div>"))

        # Tab 6: Advanced Analysis
        with tabs.children[5]:
            display(HTML("<h2>ğŸ§  Advanced Analysis</h2>"))

            # Topic Modeling
            display(HTML("<h3>ğŸ¯ Topic Modeling</h3>"))
            topics = topic_modeler.extract_topics(text)
            for topic in topics:
                display(HTML(f"<div class='info-box'>{topic}</div>"))

            # Text Complexity
            display(HTML("<h3>ğŸ“Š Text Complexity Analysis</h3>"))
            complexity = advanced_analyzer.text_complexity(text)
            complexity_df = pd.DataFrame([complexity])
            display(complexity_df)

            # Zero-shot Classification
            display(HTML("<h3>ğŸ·ï¸ Zero-Shot Classification</h3>"))
            categories = ['technology', 'sports', 'politics', 'entertainment', 'science', 'business', 'health']
            classification = advanced_analyzer.zero_shot_classification(text, categories)

            if isinstance(classification, dict):
                class_df = pd.DataFrame({
                    'Category': classification['labels'],
                    'Confidence': [f"{score:.3f}" for score in classification['scores']]
                })
                display(class_df)

                fig, ax = plt.subplots(figsize=(10, 6))
                y_pos = np.arange(len(class_df))
                ax.barh(y_pos, [float(score) for score in class_df['Confidence']])
                ax.set_yticks(y_pos)
                ax.set_yticklabels(class_df['Category'])
                ax.set_xlabel('Confidence Score')
                ax.set_title('Classification Confidence - Created by Rahul Chauhan')
                plt.tight_layout()
                plt.show()
            else:
                display(HTML(f"<div class='info-box'>{classification}</div>"))

        # Footer
        display(HTML("<hr>"))
        display(HTML("<div style='text-align: center; color: #666; margin-top: 20px;'>"
                    "<p><strong>Analysis completed successfully! ğŸ‰</strong></p>"
                    "<p>Created by Rahul Chauhan | Smart Text Analyzer</p>"
                    "</div>"))

# Connect button to analysis function
analyze_button.on_click(analyze_text)

# Display output area
display(HTML("<div class='section-header'>ğŸ“Š Analysis Results</div>"))
display(output_area)

# %%
# Export Results Section
display(HTML("<div class='section-header'>ğŸ’¾ Export & Advanced Tools</div>"))

def generate_report(text):
    """Generate a comprehensive analysis report"""
    analyzer = BasicTextAnalyzer(text)
    stats = analyzer.get_basic_stats()
    word_freq = analyzer.get_word_frequency()
    sentiment = sentiment_analyzer.get_detailed_sentiment(text)
    entities = entity_extractor.get_entity_summary(text)

    report = {
        'basic_statistics': stats,
        'word_frequency': word_freq,
        'sentiment_analysis': sentiment,
        'entities_found': len(entities['spacy_entities']),
        'entity_types': list(entities['entity_summary'].keys()) if entities['entity_summary'] else []
    }

    return report

# Export widgets
export_text = widgets.Textarea(
    value='',
    placeholder='Enter text to generate comprehensive report...',
    description='Text:',
    layout=widgets.Layout(width='100%', height='150px')
)

export_button = widgets.Button(
    description='ğŸ“„ Generate Comprehensive Report',
    button_style='warning',
    layout=widgets.Layout(width='250px', height='40px')
)

pdf_export_button = widgets.Button(
    description='ğŸ“Š Export PDF Report',
    button_style='danger',
    layout=widgets.Layout(width='200px', height='40px')
)

topic_modeling_button = widgets.Button(
    description='ğŸ§  Extract Topics',
    button_style='success',
    layout=widgets.Layout(width='150px', height='40px')
)

export_output = widgets.Output()
pdf_export_output = widgets.Output()
topic_output = widgets.Output()

def generate_export_report(b):
    with export_output:
        clear_output()

        text = export_text.value.strip()
        if not text:
            display(HTML("<div style='color: red; font-weight: bold;'>âŒ Please enter some text to generate report.</div>"))
            return

        display(HTML("<h2>ğŸ“Š Comprehensive Analysis Report</h2>"))
        report = generate_report(text)

        display(HTML("<h3>ğŸ“ˆ Basic Statistics</h3>"))
        stats_df = pd.DataFrame([report['basic_statistics']]).T
        stats_df.columns = ['Value']
        stats_df.index = [idx.replace('_', ' ').title() for idx in stats_df.index]
        display(stats_df)

        display(HTML("<h3>ğŸ˜Š Sentiment Analysis</h3>"))
        sentiment_df = pd.DataFrame([report['sentiment_analysis']['textblob']])
        display(sentiment_df)

        display(HTML("<h3>ğŸ·ï¸ Entity Information</h3>"))
        display(HTML(f"<p><strong>Total entities found:</strong> {report['entities_found']}</p>"))
        display(HTML(f"<p><strong>Entity types detected:</strong> {', '.join(report['entity_types']) if report['entity_types'] else 'None'}</p>"))

        display(HTML("<h3>ğŸ”¤ Word Frequency Analysis</h3>"))
        top_words = dict(list(report['word_frequency'].items())[:10])
        freq_df = pd.DataFrame(list(top_words.items()), columns=['Word', 'Frequency'])
        display(freq_df)

        display(HTML("<div class='success-box' style='margin-top: 20px;'>"
                    "<h4>ğŸ“‹ Report Summary</h4>"
                    f"<p><strong>Text Length:</strong> {report['basic_statistics']['word_count']} words</p>"
                    f"<p><strong>Overall Sentiment:</strong> {report['sentiment_analysis']['consensus'].title()}</p>"
                    f"<p><strong>Entities Identified:</strong> {report['entities_found']}</p>"
                    f"<p><strong>Report Generated:</strong> {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>"
                    "<p><strong>Created by:</strong> Rahul Chauhan</p>"
                    "</div>"))

def export_pdf_report(b):
    with pdf_export_output:
        clear_output()
        text = export_text.value.strip()
        if not text:
            display(HTML("<div style='color: red;'>âŒ Please enter text first</div>"))
            return
        
        display(HTML("<div class='info-box'>ğŸ”„ Generating PDF report...</div>"))
        report = generate_report(text)
        filename = f"text_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        
        try:
            result = pdf_generator.generate_comprehensive_report(text, report, filename)
            if "failed" not in result:
                # Create download link
                with open(filename, "rb") as f:
                    pdf_data = f.read()
                pdf_b64 = base64.b64encode(pdf_data).decode()
                download_link = f'<a href="data:application/pdf;base64,{pdf_b64}" download="{filename}">ğŸ“¥ Download PDF Report</a>'
                display(HTML(f"<div class='success-box'>âœ… PDF report generated successfully!<br>{download_link}</div>"))
            else:
                display(HTML(f"<div style='color: red;'>âŒ {result}</div>"))
        except Exception as e:
            display(HTML(f"<div style='color: red;'>âŒ PDF generation error: {str(e)}</div>"))

def extract_topics_export(b):
    with topic_output:
        clear_output()
        text = export_text.value.strip()
        if not text:
            display(HTML("<div style='color: red;'>âŒ Please enter text first</div>"))
            return
        
        topics = topic_modeler.extract_topics(text)
        display(HTML("<h3>ğŸ¯ Discovered Topics</h3>"))
        for topic in topics:
            display(HTML(f"<div class='info-box'>{topic}</div>"))

export_button.on_click(generate_export_report)
pdf_export_button.on_click(export_pdf_report)
topic_modeling_button.on_click(extract_topics_export)

display(widgets.VBox([
    widgets.HTML("<h3>Export & Advanced Tools</h3>"),
    export_text,
    widgets.HBox([export_button, pdf_export_button, topic_modeling_button]),
    export_output,
    pdf_export_output,
    topic_output
]))

# %%
# Final Section - About and Credits
display(HTML("<div class='section-header'>ğŸ‘¨â€ğŸ’» About This Project</div>"))

about_content = """
<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 15px; color: white;">
    <h2 style="text-align: center; margin-bottom: 20px;">ğŸ‰ Smart Text Analyzer</h2>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;">
        <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px;">
            <h3>ğŸš€ Features</h3>
            <ul>
                <li>Comprehensive Text Analysis</li>
                <li>Advanced NLP Processing</li>
                <li>Real-time Sentiment Analysis</li>
                <li>Entity Recognition</li>
                <li>Text Summarization</li>
                <li>Beautiful Visualizations</li>
                <li>Topic Modeling</li>
                <li>PDF Report Export</li>
            </ul>
        </div>

        <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px;">
            <h3>ğŸ› ï¸ Technologies</h3>
            <ul>
                <li>Python & NLP Libraries</li>
                <li>Transformers & Hugging Face</li>
                <li>spaCy for NER</li>
                <li>TextBlob for Sentiment</li>
                <li>Interactive Widgets</li>
                <li>Data Visualization</li>
                <li>PDF Generation</li>
            </ul>
        </div>
    </div>

    <div style="text-align: center; background: rgba(255,255,255,0.2); padding: 20px; border-radius: 10px;">
        <h3>ğŸ‘¨â€ğŸ’» Created by Rahul Chauhan</h3>
        <p style="font-size: 1.1em; margin-bottom: 10px;">Data Scientist & AI Enthusiast</p>
        <p style="opacity: 0.9;">This project demonstrates advanced NLP techniques and interactive data analysis capabilities.</p>
    </div>
</div>

<div style="margin-top: 30px; text-align: center;">
    <h3>ğŸ“ How to Use</h3>
    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 20px;">
        <div style="background: #e3f2fd; padding: 15px; border-radius: 10px;">
            <h4>1ï¸âƒ£ Input Text</h4>
            <p>Enter your text or choose from samples</p>
        </div>
        <div style="background: #e8f5e8; padding: 15px; border-radius: 10px;">
            <h4>2ï¸âƒ£ Analyze</h4>
            <p>Click the analyze button</p>
        </div>
        <div style="background: #fff3cd; padding: 15px; border-radius: 10px;">
            <h4>3ï¸âƒ£ Explore</h4>
            <p>Check different tabs for insights</p>
        </div>
    </div>
</div>
"""

display(HTML(about_content))

# %%
# Final success message
display(HTML("""
<div style="text-align: center; margin: 40px 0;">
    <div style="font-size: 2em; margin-bottom: 20px;">ğŸŠ</div>
    <h2 style="color: #28a745;">Your Smart Text Analyzer is Ready! ğŸš€</h2>
    <p style="font-size: 1.2em; color: #666;">Start analyzing your text using the tools above</p>
    <div style="margin-top: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
        <p><strong>Created with â¤ï¸ by Rahul Chauhan</strong></p>
        <p>Advanced NLP Tool for Text Analysis</p>
    </div>
</div>
"""))

print("\n" + "="*70)
print("ğŸ‰ SMART TEXT ANALYZER - READY TO USE!")
print("ğŸ‘¨â€ğŸ’» Created by: Rahul Chauhan")
print("ğŸ“§ For queries and contributions")
print("="*70)
